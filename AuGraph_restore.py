import numpy as np
import random
import torch
import ray
from ray.rllib.agents.ddpg import DDPGTrainer
from ray.rllib.models.catalog import ModelCatalog
from AuGraph_model_LineGraph import AuGraphModel
import Restore_path
import AuOdlConvert
from AuGraph_env_restore import AuGraphEnvRestore

seed_num = 0
np.random.seed(seed_num)
random.seed(seed_num)
torch.manual_seed(seed_num)
torch.cuda.manual_seed(seed_num)
torch.cuda.manual_seed_all(seed_num)

# 运行ray
ray.shutdown()
ray.init()
ModelCatalog.register_custom_model('augraph_model', AuGraphModel)  # 使用自定义模型

# 参数配置，把ddpg文件的粘过来就可以
config_re = {
    'env': AuGraphEnvRestore,
    'framework': 'torch',
    'seed': seed_num,
    # 'num_gpus': int(os.environ.get("RLLIB_NUM_GPUS", "0")),  # GPU
    'num_gpus': 0,  # GPU，需要<1

    # === Twin Delayed DDPG (TD3) and Soft Actor-Critic (SAC) tricks ===
    # TD3: https://spinningup.openai.com/en/latest/algorithms/td3.html
    # In addition to settings below, you can use "exploration_noise_type" and
    # "exploration_gauss_act_noise" to get IID Gaussian exploration noise
    # instead of OU exploration noise.
    # twin Q-net
    "twin_q": True,
    # delayed policy update
    "policy_delay": 1,
    # target policy smoothing
    # (this also replaces OU exploration noise with IID Gaussian exploration
    # noise, for now)
    "smooth_target_policy": True,
    # gaussian stddev of target action noise for smoothing
    "target_noise": 0.2,
    # target noise limit (bound)
    "target_noise_clip": 0.5,

    # === Evaluation ===
    # Evaluate with epsilon=0 every `evaluation_interval` training iterations.
    # The evaluation stats will be reported under the "evaluation" metric key.
    # Note that evaluation is currently not parallelized, and that for Ape-X
    # metrics are already only reported for the lowest epsilon workers.
    "evaluation_interval": None,
    # Number of episodes to run per evaluation period.
    "evaluation_duration": 10,

    # === Model ===
    # Apply a state preprocessor with spec given by the "model" config option
    # (like other RL algorithms). This is mostly useful if you have a weird
    # observation shape, like an image. Disabled by default.
    "use_state_preprocessor": True,
    # Postprocess the policy network model output with these hidden layers. If
    # use_state_preprocessor is False, then these will be the *only* hidden
    # layers in the network.
    "actor_hiddens": [128, 64],
    # Hidden layers activation of the postprocessing stage of the policy
    # network
    "actor_hidden_activation": "relu",
    # Postprocess the critic network model output with these hidden layers;
    # again, if use_state_preprocessor is True, then the state will be
    # preprocessed by the model specified with the "model" config option first.
    "critic_hiddens": [128, 64],
    # Hidden layers activation of the postprocessing state of the critic.
    "critic_hidden_activation": "relu",
    # N-step Q learning
    "n_step": 1,
    # 自定义模型
    'model': {
        'custom_model': 'augraph_model',
        "post_fcnet_hiddens": [256, 256],
        "post_fcnet_activation": 'relu',
        "fcnet_hiddens": [512],
        "fcnet_activation": 'relu',
    },

    # === Exploration ===
    "explore": True,
    "exploration_config": {
        # TD3 uses simple Gaussian noise on top of deterministic NN-output
        # actions (after a possible pure random phase of n timesteps).
        "type": "GaussianNoise",
        # For how many timesteps should we return completely random actions,
        # before we start adding (scaled) noise?
        "random_timesteps": 5000,
        # "random_timesteps": 10000,
        # Gaussian stddev of action noise for exploration.
        "stddev": 0.1,  # tune.grid_search([0.1, 0.15]),
        # Scaling settings by which the Gaussian noise is scaled before
        # being added to the actions. NOTE: The scale timesteps start only
        # after(!) any random steps have been finished.
        # By default, do not anneal over time (fixed 1.0).
        "initial_scale": 1.0,
        "final_scale": 1.0,
        "scale_timesteps": 1,
    },
    # Number of env steps to optimize for before returning
    'timesteps_per_iteration': 100,  # 每次迭代step数量
    # Extra configuration that disables exploration.
    "evaluation_config": {
        "explore": False
    },

    # === Replay buffer ===
    # Size of the replay buffer. Note that if async_updates is set, then
    # each worker will have a replay buffer of this size.
    "buffer_size": 20000,
    "replay_buffer_config": {
        "type": "MultiAgentReplayBuffer",
        "capacity": 50000,
    },
    # Set this to True, if you want the contents of your buffer(s) to be
    # stored in any saved checkpoints as well.
    # Warnings will be created if:
    # - This is True AND restoring from a checkpoint that contains no buffer
    #   data.
    # - This is False AND restoring from a checkpoint that does contain
    #   buffer data.
    "store_buffer_in_checkpoints": False,
    # If True prioritized replay buffer will be used.
    "prioritized_replay": True,
    # Alpha parameter for prioritized replay buffer.
    "prioritized_replay_alpha": 0.7,
    # Beta parameter for sampling from prioritized replay buffer.
    "prioritized_replay_beta": 0.3,
    # Epsilon to add to the TD errors when updating priorities.
    "prioritized_replay_eps": 1e-4,
    # Whether to LZ4 compress observations
    "compress_observations": False,

    # The intensity with which to update the model (vs collecting samples from
    # the env). If None, uses the "natural" value of:
    # `train_batch_size` / (`rollout_fragment_length` x `num_workers` x
    # `num_envs_per_worker`).
    # If provided, will make sure that the ratio between ts inserted into and
    # sampled from the buffer matches the given value.
    # Example:
    #   training_intensity=1000.0
    #   train_batch_size=250 rollout_fragment_length=1
    #   num_workers=1 (or 0) num_envs_per_worker=1
    #   -> natural value = 250 / 1 = 250.0
    #   -> will make sure that replay+train op will be executed 4x as
    #      often as rollout+insert op (4 * 250 = 1000).
    # See: rllib/agents/dqn/dqn.py::calculate_rr_weights for further details.
    "training_intensity": None,

    # === Optimization ===
    # Learning rate for the critic (Q-function) optimizer.
    "critic_lr": 1e-4,
    # Learning rate for the actor (policy) optimizer.
    "actor_lr": 1e-4,
    # Update the target network every `target_network_update_freq` steps.
    "target_network_update_freq": 2000,
    # Update the target by \tau * policy + (1-\tau) * target_policy
    "tau": 0.001,
    # If True, use huber loss instead of squared loss for critic network
    # Conventionally, no need to clip gradients if using a huber loss
    "use_huber": False,
    # Threshold of a huber loss
    "huber_threshold": 1.0,
    # Weights for L2 regularization
    "l2_reg": 1e-6,
    # If not None, clip gradients during optimization at this value
    "grad_clip": None,
    # How many steps of the model to sample before learning starts.
    "learning_starts": 1500,
    # Update the replay buffer with this many samples at once. Note that this
    # setting applies per-worker if num_workers > 1.
    "rollout_fragment_length": 10,
    # Size of a batched sampled from replay buffer for training. Note that
    # if async_updates is set, then each worker returns gradients for a
    # batch of this size.
    "train_batch_size": 256,

    # === Parallelism ===
    # Number of workers for collecting samples with. This only makes sense
    # to increase if your environment is particularly slow to sample, or if
    # you're using the Async or Ape-X optimizers.
    "num_workers": 0,
    # Whether to compute priorities on workers.
    "worker_side_prioritization": False,
    # Prevent reporting frequency from going lower than this time span.
    "min_time_s_per_reporting": 1,
    # Experimental flag.
    # If True, the execution plan API will not be used. Instead,
    # a Trainer's `training_iteration` method will be called as-is each
    # training iteration.
    "_disable_execution_plan_api": False,
}

path = Restore_path.path

# 恢复经过训练的agent
agent = DDPGTrainer(config=config_re, env=AuGraphEnvRestore)
agent.restore(path)
env = AuGraphEnvRestore({})
episode_reward = 0
done = False
obs = env.reset()

count = 0  # 可以在测试的时候也多跑几次，取一个最好的，把explore开启之后是能选到训练时最好的路径
reward_max = -1000
reward_list = []  # 奖励集合
while count < 20:
    while not done:
        action = agent.compute_action(obs)
        # action = agent.compute_action(obs,explore=False)
        obs, reward, done, info = env.step(action)
        # print("Action:", action*1000)
        # print("State:", obs)
        # print("Reward:", reward)
        print('------')
        episode_reward += reward
        print("Total Reward:", episode_reward)

    if episode_reward >= reward_max:  # 只把最好结果的路径记录下来
        reward_max = episode_reward
        # print('reward_max', reward_max)
        AuOdlConvert.odl_result(AuGraphEnvRestore.au_edge_list)
    #
    #     # print("RWA", AuOdlConvert.result_rwa_phy)
    #     path_count = 0
    #     path_length = []
    #     while path_count < len(AuOdlConvert.result_rwa_phy):
    #         length = int((len(AuOdlConvert.result_rwa_phy[path_count]) - 1) / 2) + int(
    #             (len(AuOdlConvert.result_rwa_phy[path_count + 1]) - 1) / 2)
    #         path_length.append(length)
    #         # path_length.append(int((len(AuOdlConvert.result_rwa_phy[path_count])-1)/2) + int((len(AuOdlConvert.result_rwa_phy[path_count+1])-1)/2))
    #         path_count += 2
    #     print("路径长度")
    #     print("ADMIRE")
    #     for pathi in path_length:
    #         print(pathi)
    #
    #     print("累计虚拟拓扑跳数")
    #     print(virtual_hop_cumulate)
    #     print("虚拟拓扑跳数50")
    #     print("ADMIRE")
    #     index = 1
    #     while index < len(virtual_hop_cumulate_list):
    #         virtual_hop_cumulate_list[index] += virtual_hop_cumulate_list[index - 1]
    #         index += 2
    #
    #     index = 1
    #     while index < len(virtual_hop_cumulate_list):
    #         print(virtual_hop_cumulate_list[index])
    #         index += 2
    # print(Service.path1)

    # with open('rwa_phy.txt', 'w') as f:
    #     for i in range(len(AuOdlConvert.result_rwa_phy)):
    #         f.write(str(AuOdlConvert.result_rwa_phy[i]))
    #         f.write('\n')
    #     f.close()
    #
    # with open('rwa_vir.txt', 'w') as file:
    #     for i in range(len(AuOdlConvert.result_rwa_vir)):
    #         file.write(str(AuOdlConvert.result_rwa_vir[i]))
    #         file.write('\n')
    #     file.close()
    #
    # # 写物理链路
    # with open('phyLinks-GA.txt', 'w') as file:
    #     for data in Database.links_physical:
    #         np.savetxt(file, data, fmt='%.3f', delimiter='\t')
    #         # file.write('\n')
    #     file.close()
    #
    # with open('link_vir.txt', 'w') as file:
    #     for k in range(len(AuGraph.links_virtual_list)):
    #         file.write(str(AuGraph.links_virtual_list[k]))
    #         file.write('\n')
    #     file.close()
    # #
    # with open('odl_iog.txt', 'w') as file:
    #     for i in range(len(AuOdlConvert.result_odl)):
    #         file.write(str(AuOdlConvert.result_odl[i]))
    #         file.write('\n')
    #     file.close()
    #
    # linkmsg_phy = [[] for _ in range(Database.time)]
    # row, col = Database.graph_connect.shape
    # for t in range(Database.time):
    #     for i in range(row):
    #         for j in range(col):
    #             if Database.graph_connect[i][j] == 1:
    #                 linkmsg_phy[t].append({'src': i, 'dst': j, str(t)+'_hour':Database.links_physical[t*3:(t+1)*3, i, j].tolist()})
    #
    # for t in range(Database.time):
    #     with open(str(t) +'_hour_links.txt', 'w') as file:
    #         for k in range(len(linkmsg_phy[t])):
    #             file.write(str(linkmsg_phy[t][k]))
    #             file.write('\n')
    #         file.close()

    reward_list.append(episode_reward)
    count += 1
    episode_reward = 0
    obs = env.reset()
    done = False

print("奖励列表", reward_list)
# print("Total Reward:", episode_reward)
# print("使用波长", Compute.compute(Database.links_physical))
